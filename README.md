# 104å·¥ä½œå…§å®¹åˆ†æ
## å‰è¨€

å¦‚æœæœ‰ä¸€å€‹å­¸ç¿’çš„ç›®æ¨™å°æ–¼åœ¨å­¸çš„å­¸ç”Ÿç„¡ç–‘æ˜¯æœ€å¤§çš„å¹«åŠ©ï¼Œå¦‚æœèƒ½äº†è§£æœªä¾†å·¥ä½œæ‰€éœ€è¦å…·å‚™çš„æŠ€èƒ½æˆ–æ˜¯èƒ½åŠ›ï¼Œæƒ³å¿…å¯ä»¥å¢åŠ å­¸ç¿’çš„å‹•åŠ›ï¼Œè€Œæˆ‘æœ¬èº«æ˜¯å°±è®€è³‡æ–™åˆ†æç›¸é—œçš„é ˜åŸŸï¼Œå› æ­¤æˆ‘å°æ–¼åœ¨104äººåŠ›éŠ€è¡Œä¸Šçš„æ•¸æ“šåˆ†æè·ç¼ºé€²è¡Œäº†åˆ†æï¼Œå¸Œæœ›èƒ½é€éåˆ†æçµæœäº†è§£ä»¥ä¸‹å¹¾é»:

- é¸æ“‡çš„å·¥ä½œé€šå¸¸å±¬æ–¼å“ªç¨®è·å‹™é¡åˆ¥ä»¥åŠè¦æ±‚çš„å·¥ä½œæŠ€èƒ½ç‚ºä½•<br>
- è¦æ±‚çš„å­¸æ­·åˆ†å¸ƒ<br>
- è¦æ±‚æ“…é•·çš„å·¥å…·<br>
- äº†è§£æ‰€é¸å·¥ä½œçš„å·¥ä½œæ€§è³ªåŠè©²å…·å‚™çš„èƒ½åŠ›<br>

## (ä¸€)  104äººåŠ›éŠ€è¡Œæ•¸æ“šåˆ†æè·ç¼ºå…§å®¹æŠ“å–


```![142761959-5ede1036-ed79-4829-b959-47cce02ec1ed](https://user-images.githubusercontent.com/44692570/142761982-ede34c2b-219f-41b6-8e4e-d8acc22865cd.png)

import job104_spider

# å–å¾—å·¥ä½œå…§å®¹/job_type=æŸ¥è©¢å…§å®¹/page=æŸ¥è©¢é æ•¸
data = job104_spider.get_job_context('æ•¸æ“šåˆ†æ',18)
data.head(5)
```
![image](https://user-images.githubusercontent.com/44692570/142760144-38777eeb-c10a-40d6-a324-c5d42d74a78e.png)

é€™é‚Šæˆ‘å€‘æŠ“å–äº†356å€‹å·¥ä½œåç¨±æœ‰æåˆ°æ•¸æ“šåˆ†æçš„è·ç¼ºã€‚
## (äºŒ) è·å‹™é¡åˆ¥&å·¥ä½œæŠ€èƒ½è¦æ±‚

```
from collections import Counter
import pandas as pd
import numpy as np

#è·å‹™é¡åˆ¥æ’å

data['è·å‹™é¡åˆ¥2'] = data.è·å‹™é¡åˆ¥.apply(lambda x: x.split('ã€')) #åˆ†è©
category_test = data.è·å‹™é¡åˆ¥2
category_cnt = Counter([word for sent in category_test for word in sent])  # è¨ˆç®—æ¬¡æ•¸
category_fre_most=category_cnt.most_common(10) #é¡¯ç¤ºå‰10åæœ€å¸¸å‡ºç¾çš„è©

Job_category_df=pd.DataFrame({'è·å‹™é¡åˆ¥': [key for key, values in category_fre_most],
              'å‡ºç¾æ¬¡æ•¸': [values for key, values in category_fre_most]},index=np.arange(1,11))

#å·¥ä½œæŠ€èƒ½æ’å

data['å·¥ä½œæŠ€èƒ½2'] = data.å·¥ä½œæŠ€èƒ½.apply(lambda x: x.split('ã€'))
Job_ability_text = data.å·¥ä½œæŠ€èƒ½2
Job_ability_text_cnt = Counter([word for sent in Job_ability_text for word in sent])  # è¨ˆç®—æ¬¡æ•¸
Job_ability_fre_most=Job_ability_text_cnt.most_common(11)[1:]    #ç¬¬ä¸€å€‹ç‚º''

Job_ability_df=pd.DataFrame({'å·¥ä½œæŠ€èƒ½': [key for key, values in Job_ability_fre_most],
              'å‡ºç¾æ¬¡æ•¸': [values for key, values in Job_ability_fre_most]},index=np.arange(1,11))

df_concat=pd.concat([Job_category_df,Job_ability_df],axis=1)
df_concat
```
![image](https://user-images.githubusercontent.com/44692570/142760493-85da98e2-b9dd-440f-96a8-aee331219a2a.png)

å¾è³‡æ–™ä¸Šä¾†çœ‹ï¼Œå¯ä»¥ç™¼ç¾èªªæ•¸æ“šåˆ†æçš„è·ç¼ºèˆ‡<b>å¸‚å ´èª¿æŸ¥èˆ‡åˆ†æ</b>é€™å¡Šæ¯æ¯ç›¸é—œã€‚

## (äºŒ) å­¸æ­·è¦æ±‚

```
#æœ€ä½å­¸æ­·è¦æ±‚åˆ†å¸ƒ

def Education_require(sentence):
    
    if 'ä¸æ‹˜' in sentence:
        return 'ä¸æ‹˜'

    elif 'é«˜ä¸­' in sentence:
        return 'é«˜ä¸­'

    elif 'å°ˆç§‘' in sentence or 'å¤§å­¸' in sentence:    
        return 'å°ˆç§‘ã€å¤§å­¸'
    
    elif 'ç¢©å£«' in sentence:
        return 'ç¢©å£«'
    
    elif 'åšå£«' in sentence:
        return 'åšå£«'

data['æœ€ä½å­¸æ­·è¦æ±‚']=data.å­¸æ­·è¦æ±‚.apply(lambda x :Education_require(x))


#æœ€ä½å­¸æ­·è¦æ±‚åˆ†å¸ƒé•·æ¢åœ–

import numpy as np
import matplotlib.pyplot as plt
 
plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta'] 
height = data.æœ€ä½å­¸æ­·è¦æ±‚.value_counts().values
bars = data.æœ€ä½å­¸æ­·è¦æ±‚.value_counts().index
x_pos = np.arange(len(bars))

plt.title('æœ€ä½å­¸æ­·è¦æ±‚')
plt.bar(x_pos, height, color='lightblue',  edgecolor='blue')
plt.xticks(x_pos, bars)
plt.ylabel('å‡ºç¾æ¬¡æ•¸')

for x, y in enumerate(height):
    plt.text(x, y, '%s' % y, ha='center',va='bottom')

plt.show()
```
![image](https://user-images.githubusercontent.com/44692570/142760087-ff90a17e-4b4f-4e8e-8a75-e8593ed30477.png)

æ•¸æ“šåˆ†æé€™ä¸€è¡Œå°æ–¼å­¸æ­·çš„è¦æ±‚æ²’æœ‰æƒ³åƒä¸­çš„é«˜ï¼Œåˆæˆ–è€…èªªæœƒåœ¨104ä¸Šé–‹è·ç¼ºçš„å·¥ä½œé€šå¸¸å°å­¸æ­·çš„è¦æ±‚å¯èƒ½ç›¸å°æœƒæ¯”è¼ƒæ²’é‚£éº¼è‹›åˆ»ã€‚

# (ä¸‰) è¦æ±‚æ“…é•·çš„å·¥å…·çš„æ’å

```
import jieba
from collections import Counter

data['æ“…é•·å·¥å…·'] = data.æ“…é•·å·¥å…·.apply(lambda x: x.replace(u'\u200b', '').replace(u'u3000','')) #åˆªé™¤ç‰¹æ®Šå­—ç¬¦
data['æ“…é•·å·¥å…·2'] = data.æ“…é•·å·¥å…·.apply(lambda x: x.split('ã€'))  #åˆ†è©
tool_text = data.æ“…é•·å·¥å…·2
tool_cnt = Counter([word for sent in tool_text for word in sent])  # è¨ˆç®—æ¬¡æ•¸
tool_fre_most=tool_cnt.most_common(11)[1:] #å‰10åæœ€å¸¸å‡ºç¾çš„è©

#ç†±é–€å·¥å…·æ’è¡Œé•·æ¢åœ–

import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']
height = [values for key, values in tool_fre_most]
bars = [key for key, values in tool_fre_most]
x_pos = np.arange(len(bars))
plt.figure(figsize=(10, 4))
plt.title('ç†±é–€å·¥å…·æ’å')
plt.bar(x_pos, height, color='#9999FF')
plt.xticks(x_pos, bars, rotation=45)
plt.ylabel('å‡ºç¾æ¬¡æ•¸')

for x, y in enumerate(height):
    plt.text(x, y, '%s' % y, ha='center',va='bottom')

plt.show()
```
![image](https://user-images.githubusercontent.com/44692570/142760562-3d52f37c-6868-409f-9920-bae306fdc7b0.png)

èº«ç‚ºç•¶ä¸‹ç†±é–€ç¨‹å¼èªè¨€çš„Pythonæ‹¿ä¸‹äº†ç¬¬ä¸€åï¼Œè¶…éä¸‰åˆ†ä¹‹ä¸€çš„è·ç¼ºéœ€è¦æœƒä½¿ç”¨Pythonï¼Œè€ŒExcelã€PowerPointåŠWordä¸‰æ¨£åŸºæœ¬åˆ†æå·¥å…·ä¹Ÿåœ¨å‰10åä¸­ä½”äº†3å€‹ã€‚MS SQLå‰‡æ˜¯æ‰“æ•—å…¶ä»–SQLç¨è‡ªæ“ é€²å‰10ä¸¦æ’åºç¬¬ä¸‰ã€‚<br>

åœ¨å‰é¢æˆ‘å€‘æœ‰ç™¼ç¾èªªæ•¸æ“šåˆ†æçš„è·ç¼ºèˆ‡<b>å¸‚å ´èª¿æŸ¥èˆ‡åˆ†æ</b>é€™å¡Šæœ‰å¾ˆå¤§çš„é—œè¯ï¼Œå› æ­¤å°æ–¼è£½ä½œå ±è¡¨åŠç¶²è·¯åˆ†æçš„å·¥å…·ä¹Ÿæœƒæœ‰ä¸å°çš„éœ€æ±‚ï¼Œè€ŒTableauã€Power BIåŠGoogle Analyticsçš„ä¸Šæ¦œä¹Ÿè¨±å¯ä»¥é–“æ¥è­‰å¯¦æ•¸æ“šåˆ†æèˆ‡å¸‚å ´èª¿æŸ¥åˆ†æçš„é—œè¯

# (å››) æ¢è¨è©²å…·å‚™çš„å·¥å…·çµ„åˆ

```
#ä½¿ç”¨é—œè¯è¦å‰‡æ‰¾å‡ºå®¹æ˜“åŒæ™‚éœ€è¦å…·å‚™çš„å·¥å…·

from mlxtend.preprocessing import TransactionEncoder    #ç·¨ç¢¼å™¨
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

te = TransactionEncoder()
te_ary = te.fit(data['æ“…é•·å·¥å…·2']).transform(data['æ“…é•·å·¥å…·2'])
tool_df = pd.DataFrame(te_ary, columns=te.columns_)
tool_df.head(5)


frequent_items = apriori(tool_df, min_support=0.1, use_colnames=True)   #min_supportç‚º0.1
association_df = association_rules(frequent_items,metric='confidence', min_threshold=0.3).sort_values(by='confidence', ascending=False)[
    ['antecedents', 'consequents', 'support', 'confidence', 'lift']].reset_index(drop=True) #min_confidenceç‚º0.3

association_df
```
![image](https://user-images.githubusercontent.com/44692570/142761959-5ede1036-ed79-4829-b959-47cce02ec1ed.png)

é€™è£¡æˆ‘ä½¿ç”¨Aprioriæ¼”ç®—æ³•å»æ‰¾å‡ºæ•¸æ“šåˆ†æè·ç¼ºä¸­å¸¸å‡ºç¾çš„æ“…é•·å·¥å…·çµ„åˆï¼Œä¸‹é¢å…ˆç°¡å–®çš„è§£é‡‹ä¸‹æ¬„ä½åç¨±æ‰€ä»£è¡¨çš„æ„æ€:<br>

- antecedents(å‰é …):é¡ä¼¼æ–¼å‰å› å¾Œæœä¸­çš„åŸå› <br>
- consequents(å¾Œé …):é¡ä¼¼æ–¼å‰å› å¾Œæœä¸­çš„å¾Œæœ<br>
- support(æ”¯æŒåº¦):ä»¥ç¬¬ä¸€ç­†è³‡æ–™ç‚ºä¾‹ï¼ŒPowerPointèˆ‡ExcelåŒæ™‚å‡ºç¾åœ¨æ¨£æœ¬ä¸­çš„æ©Ÿç‡ç‚º0.148876ï¼Œä¹Ÿå°±æ˜¯èªª356å€‹è·ç¼ºæœ‰53å€‹è·ç¼ºè¦æ±‚ä½ åŒæ™‚æ“…é•·PowerPointåŠExcel<br>
- confidence(ä¿¡è³´åº¦):ä»¥ç¬¬ä¸€ç­†è³‡æ–™ç‚ºä¾‹ï¼Œåœ¨è¦æ±‚éœ€è¦PowerPointçš„è·ä½ï¼Œæœ‰100%éœ€è¦ä½ æœƒExcel<br>
- lift(æå‡åº¦):è¨ˆç®—æ–¹å¼ç‚ºconfidence(A->B)/support(B)ï¼Œå°æ–¼1ç‚ºè² ç›¸é—œï¼Œç­‰æ–¼1ç‚ºç¨ç«‹ï¼Œå¤§æ–¼1å‰‡æ˜¯æ­£ç›¸é—œ<br>

äº†è§£Aprioriæ¼”ç®—æ³•å¾Œä¾†çœ‹åˆ†æçµæœï¼Œæˆ‘å€‘å¯ä»¥æ­¸é¡å‡ºä¸‹é¢å››ç¨®çµ„åˆï¼Œåˆ†åˆ¥ç‚º:<br>

### 1. Excelã€PowerPointã€Word
### 2. Rã€Python
### 3. Pythonã€MS SQL
### 4. Tableauã€Python 

é‚£æœƒç”¨åˆ°ç¬¬ä¸€ç¨®çµ„åˆçš„å·¥ä½œæ‡‰è©²æ¯”è¼ƒåå‘æ–‡æ›¸è™•ç†ï¼Œç¬¬äºŒã€ä¸‰ç¨®çµ„åˆå°æ–¼codingèƒ½åŠ›çš„è¦æ±‚ç›¸å°æ¯”è¼ƒé«˜ï¼Œç¬¬å››ç¨®çµ„åˆå‰‡åå‘æ³¨é‡å°‡æ•¸æ“šè¦–è¦ºåŒ–çš„èƒ½åŠ›ã€‚

# (äº”) æ¢è¨å·¥ä½œæ€§è³ªåŠè©²å…·å‚™çš„èƒ½åŠ›

```
#æ–‡å­—é›²

import jieba
import re

stopwords = [k.strip() for k in open('åœç”¨è©.txt', encoding='utf-8') if k.strip() != '']    #åœç”¨è©

jieba.set_dictionary('dict.txt.big')  # ç¹é«”ä¸­æ–‡æª”

#å°‡å·¥å…·çš„å°ˆæœ‰åè©åŠ å…¥è‡ªå®šç¾©è©
for keys in tool_cnt.keys():
    jieba.add_word(keys)

#åˆªé™¤æ¨™æ¨™é»ç¬¦è™Ÿ
def clean_Punctuation(text):
    text=re.sub(r'[^\w\s]','',text) #åˆªé™¤æ¨™æ¨™é»ç¬¦è™Ÿ
    text=text.replace('\n','').replace('\r','').replace('\t','').replace(' ','').replace('[','').replace(']','')
    return text
#æ–·è©/å»é™¤æ¨™é»ç¬¦è™Ÿ/å»é™¤åœç”¨å­—
def text_cut(sentence):
    sentence_cut=[word for word in jieba.lcut(clean_Punctuation(sentence)) if word not in stopwords]

    return sentence_cut

job_content= data.å·¥ä½œå…§å®¹ 
job_content_cut=[word for sent in job_content for word in set(text_cut(sent)) if word not in 'æ•¸æ“šåˆ†æ']  #æ–·è©
job_content_cut_cnt=Counter([word for word in job_content_cut])   #è¨ˆç®—æ¬¡æ•¸

#æ–‡å­—é›²ä½œåœ–

import wordcloud # è©é›²å±•ç¤ºåº«
from PIL import Image # å½±åƒè™•ç†åº«
import matplotlib.pyplot as plt # å½±åƒå±•ç¤ºåº«


#æ ¹æ“šå–®è©åŠå…¶é »ç‡ç”Ÿæˆè©é›²
wc = wordcloud.WordCloud(
    font_path='NotoSerifCJKtc-Medium.otf', # è¨­å®šå­—å‹æ ¼å¼
    max_words=40, # æœ€å¤šé¡¯ç¤ºè©æ•¸
    max_font_size=180, # å­—å‹æœ€å¤§å€¼
    background_color='white',
    width=800, height=600,
)

wc.generate_from_frequencies(job_content_cut_cnt) # å¾å­—å…¸ç”Ÿæˆè©é›²
plt.imshow(wc) # é¡¯ç¤ºè©é›²
plt.axis('off') # é—œé–‰åº§æ¨™è»¸
plt.show() # é¡¯ç¤ºå½±åƒ
```
![image](https://user-images.githubusercontent.com/44692570/142763302-e91f8212-3122-4112-94c0-3bcd3aab3e4e.png)

é€™é‚Šæˆ‘å°‡å·¥ä½œå…§å®¹çš„æ¬„ä½åšæ–·è©ï¼Œä¸¦ä½¿ç”¨set()å‡½æ•¸å»é™¤é‡è¤‡çš„è©ï¼Œä»¥é˜²ä¸€å€‹è·ç¼ºæåˆ°ä¸€å€‹è©çš„é »ç‡å¤ªé »ç¹ï¼Œè€Œé€™å€‹è©åœ¨å…¶ä»–è·ç¼ºå»é®®å°‘å‡ºç¾ï¼Œå°è‡´å‘ˆç¾çµæœä¸å¦‚é æœŸï¼Œä¹Ÿå°±æ˜¯èªªè‹¥æœ‰ä¸€å€‹è·ç¼ºçš„å·¥ä½œå…§å®¹æåˆ°ã€Œå¤§æ•¸æ“šçš„æ™‚ä»£ï¼Œæˆ‘å€‘éœ€è¦å¤§æ•¸æ“šçš„äººæ‰ã€ï¼Œé›–ç„¶ä»–æåˆ°å¤§æ•¸æ“šä¸€è©äºŒæ¬¡ï¼Œä½†åœ¨æˆ‘é€™é‚Šåªæœƒè¨˜éŒ„ä¸€æ¬¡ã€‚<br>

å¾æ–‡å­—é›²åœ–ä¸Šå¯ä»¥çœ‹åˆ°é™¤äº†<b>å ±è¡¨ã€è³‡æ–™åº«ã€ç³»çµ±</b>ç­‰æ¯”è¼ƒåç¡¬å¯¦åŠ›çš„è©ï¼Œä¹Ÿå¯ä»¥çœ‹åˆ°<b>å”åŠ©ã€å°ˆæ¡ˆã€æºé€šã€åœ˜éšŠã€åˆä½œ</b>ç­‰èˆ‡åœ˜éšŠåˆä½œç›¸é—œçš„è©ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥å¾—çŸ¥ï¼Œèˆ‡äººäº¤éš›æºé€šçš„èƒ½åŠ›åœ¨æ•¸æ“šåˆ†æç›¸é—œå·¥ä½œä¹Ÿæ˜¯éå¸¸é‡è¦çš„ã€‚å¦å¤–<b>ç¶“é©—ã€ç†Ÿæ‚‰</b>äºŒè©å‡ºç¾çš„é »ç‡ä¹Ÿå¾ˆé«˜ï¼Œé€™è »å¥½ç†è§£ï¼Œç•¢ç«Ÿåœ¨æˆ‘å€‘è³‡è¨Šæ¥­ç¶“é©—çš„ç´¯ç©èˆ‡å·¥å…·çš„ç†Ÿæ‚‰åº¦æ˜¯éå¸¸é‡è¦çš„ï¼Œåœ¨å¤§å­¸åƒèˆ‡ç”¢å¯¦ç¿’æˆ–è€…æ˜¯ç”¢å­¸åˆä½œæ˜¯ä¸€å€‹ç´¯ç©ç¶“é©—èˆ‡ç†Ÿæ‚‰å·¥å…·çš„å¥½æ–¹æ³•ã€‚

# (å…­) è‹¥å°‡å·¥ä½œçš„å…§å®¹åˆ†æˆ3å€‹ä¸»é¡Œ

```
#ä¸»é¡Œåˆ†æ

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# å°‡å­—ä¸²åˆ‡å‰²å¾Œä¾ç…§é€—é»åˆ†å‰²
data['å·¥ä½œå…§å®¹2'] = data.å·¥ä½œå…§å®¹.apply(lambda x: str(text_cut(x)).replace(
    '[', '').replace(']"', '').replace(',', '').replace('\'', ''))

     
data['å…¶ä»–æ¢ä»¶2'] = data.å…¶ä»–æ¢ä»¶.apply(lambda x: str(text_cut(x)).replace(
    '[', '').replace(']"', '').replace(',', '').replace('\'', ''))

#æ¨¡å‹è¨“ç·´
tf_vectorizer = CountVectorizer(strip_accents='unicode',
                                max_df=0.5,
                                min_df=10)  
tf = tf_vectorizer.fit_transform(data['å·¥ä½œå…§å®¹2'])

lda = LatentDirichletAllocation(n_components=3, max_iter=50,    #n_components=3/åˆ†æˆä¸‰å€‹ä¸»é¡Œ
                                learning_method='online',
                                learning_offset=50.,
                                random_state=0)

lda.fit(tf)

def print_topic(feature_names, n_top_words):
    for topic_idx, topic in enumerate(lda.components_):

        print("ä¸»é¡Œ %d:" % (topic_idx+1))
        print(" ".join([feature_names[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
        print()
    
tf_feature_names = tf_vectorizer.get_feature_names()
print_topic(tf_feature_names, 20)
```
![image](https://user-images.githubusercontent.com/44692570/142764860-7b5fb994-9f98-4df4-97dc-241692b044a4.png)

é€™é‚Šä½¿ç”¨ scikit-learnçš„ LatentDirichletAllocationå¥—ä»¶å°‡å·¥ä½œå…§å®¹åˆ†æˆä¸‰å€‹ä¸»é¡Œã€‚ä¸»é¡Œ1åƒæ˜¯é‡å°ä½¿ç”¨è€…é«”é©—æ–¹é¢ï¼Œä¸»é¡Œ2åå‘å°ˆæ¡ˆçš„è¦åŠƒèˆ‡åŸ·è¡Œï¼Œä¸»é¡Œ3å‰‡æ˜¯å·¥å…·çš„ä½¿ç”¨æ–¹é¢ã€‚

# ç¸½çµ

é™¤äº†æ•¸æ“šåˆ†æè·ç¼ºä»¥å¤–ï¼Œæˆ‘é‚„æœ‰å¦å¤–è·‘äº†å…¶ä»–å¹¾å€‹é—œéµå­—ï¼Œä¾‹å¦‚è³‡æ–™ç§‘å­¸ï¼Œè³‡æ–™å·¥ç¨‹ç­‰ç­‰ã€‚ç¶“éä¸Šé¢çš„æ•¸æ“šåŠèˆ‡å…¶ä»–é—œéµå­—æ¯”è¼ƒå¾Œï¼Œå¯ä»¥ç™¼ç¾æ•¸æ“šåˆ†æç®—æ˜¯è³‡æ–™ç§‘å­¸é ˜åŸŸä¸­å…¥é–€é–€æª»æ¯”è¼ƒä½çš„è·ç¼ºã€‚<br>
å°±å­¸æ­·è¦æ±‚è€Œè¨€ï¼Œè³‡æ–™ç§‘å­¸è·ç¼ºä¸­éœ€è¦æœ‰ç¢©å£«å­¸æ­·çš„æ¯”ä¾‹å°±é é«˜æ–¼æ•¸æ“šåˆ†æè·ç¼ºã€‚è³‡æ–™ç§‘å­¸è·ç¼ºä¸­è·å‹™é¡åˆ¥çš„æ­¸é¡å‰‡ä»¥æ¼”ç®—æ³•é–‹ç™¼å·¥ç¨‹å¸«å‡ºç¾æ¬¡æ•¸æœ€å¤šã€‚å†ä¾†æ˜¯å·¥å…·ä½¿ç”¨çš„å·®ç•°ï¼Œè³‡æ–™ç§‘å­¸è·ç¼ºç†±é–€ä½¿ç”¨çš„å·¥å…·å‰10åä¸­å·²ç¶“ä¸è¦‹PowerPointåŠWordçš„è¹¤å½±ï¼Œå–è€Œä»£ä¹‹çš„å‰‡æ˜¯Javaä»¥åŠGitã€‚è€Œæœ€å¤§çš„ä¸åŒå‰‡åœ¨æ–¼å·¥ä½œçš„å…§å®¹ï¼Œæ•¸æ“šåˆ†æè·ç¼ºçš„å·¥ä½œå…§å®¹è‘—é‡æ–¼è³‡æ–™çš„åˆ†æï¼Œè³‡æ–™ç§‘å­¸è·ç¼ºé‡è¦–æ¨¡å‹ã€æ¼”ç®—æ³•çš„è¨­è¨ˆï¼Œæ©Ÿå™¨å­¸ç¿’çš„èƒ½åŠ›ã€‚è‹¥æƒ³å¾äº‹è³‡æ–™ç§‘å­¸ç›¸é—œçš„å·¥ä½œï¼Œçµ±è¨ˆçš„èƒ½åŠ›æ©Ÿå™¨å­¸ç¿’çš„åŠŸåŠ›æ˜¯ä¸å¯æˆ–ç¼ºçš„<br>

### é‚£éº¼ä»¥ä¸Šç‚ºå€‹äººå°æ–¼æ•¸æ“šåˆ†æè·ç¼ºçš„ä¸€äº›åˆ†æèˆ‡è¦‹è§£ï¼Œè¦ºå¾—å°ä½ æœ‰å¹«åŠ©çš„éº»ç…©å¹«å¿™é»å€‹è®šğŸ‘



